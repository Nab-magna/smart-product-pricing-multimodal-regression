{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13369980,"sourceType":"datasetVersion","datasetId":8481869}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:05:00.465921Z","iopub.execute_input":"2025-10-13T14:05:00.466074Z","iopub.status.idle":"2025-10-13T14:05:00.800811Z","shell.execute_reply.started":"2025-10-13T14:05:00.466059Z","shell.execute_reply":"2025-10-13T14:05:00.800162Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/e5-embedd/train_e5_embeddings.npy\n/kaggle/input/e5-embedd/test_e5_embeddings.npy\n/kaggle/input/e5-embedd/train.csv\n/kaggle/input/e5-embedd/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install catboost --quiet\n!pip install XGBoost --quiet\n!pip install LightGBM --quiet\n!pip install optuna -q\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:06:28.198575Z","iopub.execute_input":"2025-10-13T14:06:28.199332Z","iopub.status.idle":"2025-10-13T14:06:40.519270Z","shell.execute_reply.started":"2025-10-13T14:06:28.199302Z","shell.execute_reply":"2025-10-13T14:06:40.518448Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\n\n# --- Load training and test data ---\ntrain_df = pd.read_csv(\"/kaggle/input/e5-embedd/train.csv\")   # replace with correct path if needed\ntest_df  = pd.read_csv(\"/kaggle/input/e5-embedd/test.csv\")    # replace with correct path if needed\n\n# --- Define target and ID columns ---\ntarget_col = \"price\"       # target variable\nid_col     = \"sample_id\"   # unique ID column\n\nprint(\"âœ… Data loaded successfully!\")\nprint(f\"Train shape: {train_df.shape}\")\nprint(f\"Test shape: {test_df.shape}\")\nprint(f\"Target column: {target_col}\")\nprint(f\"ID column: {id_col}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:05:59.074900Z","iopub.execute_input":"2025-10-13T14:05:59.075209Z","iopub.status.idle":"2025-10-13T14:06:03.899085Z","shell.execute_reply.started":"2025-10-13T14:05:59.075185Z","shell.execute_reply":"2025-10-13T14:06:03.898311Z"}},"outputs":[{"name":"stdout","text":"âœ… Data loaded successfully!\nTrain shape: (75000, 4)\nTest shape: (75000, 3)\nTarget column: price\nID column: sample_id\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ===================================================================\n# MAXIMUM SMAPE REDUCTION: ADVANCED FEATURE ENGINEERING +\n# QUANTILE REGRESSION + PSEUDO-LABELING + STACKING\n# OPTIMIZED VERSION: 80/20 TRAIN-TEST SPLIT (No K-Fold)\n# Target: Reduce SMAPE from 56 to ~40\n# ===================================================================\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import Ridge, QuantileRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom scipy.stats import skew, kurtosis\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"=\"*80)\nprint(\"ðŸš€ ADVANCED PRICE PREDICTION PIPELINE - MAXIMUM SMAPE REDUCTION\")\nprint(\"âš¡ FAST MODE: 80/20 TRAIN-TEST SPLIT (No K-Fold)\")\nprint(\"=\"*80)\n\n# -----------------------------\n# Define pseudo-labeling function BEFORE use\n# -----------------------------\ndef pseudo_label_test_data(train_X, train_y, test_X, confidence_threshold=0.80):\n    \"\"\"Generate high-confidence pseudo-labels for test data\"\"\"\n    \n    quick_models = [\n        LGBMRegressor(n_estimators=500, learning_rate=0.05, verbose=-1, random_state=42),\n        XGBRegressor(n_estimators=500, learning_rate=0.05, verbosity=0, random_state=42)\n    ]\n    \n    test_preds = []\n    for model in quick_models:\n        model.fit(train_X, train_y)\n        test_preds.append(model.predict(test_X))\n    \n    test_pred_mean = np.mean(test_preds, axis=0)\n    test_pred_std = np.std(test_preds, axis=0)\n    \n    confidence = 1 / (1 + test_pred_std)\n    high_conf_mask = confidence > np.quantile(confidence, confidence_threshold)\n    \n    pseudo_X = test_X[high_conf_mask]\n    pseudo_y = test_pred_mean[high_conf_mask]\n    \n    print(f\"  âœ“ Generated {len(pseudo_y)} pseudo-labels ({len(pseudo_y)/len(test_X)*100:.1f}% of test data)\")\n    print(f\"  âœ“ Mean confidence: {confidence[high_conf_mask].mean():.3f}\")\n    \n    return pseudo_X, pseudo_y\n\n# -----------------------------\n# 1. LOAD AND PREPARE BASE FEATURES\n# -----------------------------\nprint(\"\\n[Step 1/6] Loading embeddings and base features...\")\ntrain_emb_file = \"/kaggle/input/e5-embedd/train_e5_embeddings.npy\"\ntest_emb_file  = \"/kaggle/input/e5-embedd/test_e5_embeddings.npy\"\n\ntrain_embeddings = np.load(train_emb_file)\ntest_embeddings  = np.load(test_emb_file)\n\ntrain_embedding_df = pd.DataFrame(train_embeddings, columns=[f'e5_emb_{i}' for i in range(train_embeddings.shape[1])])\ntest_embedding_df  = pd.DataFrame(test_embeddings,  columns=[f'e5_emb_{i}' for i in range(test_embeddings.shape[1])])\n\nfeature_cols = [col for col in train_df.columns if any(\n    suffix in col for suffix in ['_char_count', '_word_count', '_avg_word_length', \n                                 '_sentence_count', '_uppercase_ratio', '_digit_ratio',\n                                 '_special_char_ratio', '_unique_word_ratio']\n)]\n\nprint(f\"âœ“ Loaded {train_embeddings.shape[1]} embedding dimensions\")\nprint(f\"âœ“ Loaded {len(feature_cols)} text statistics features\")\n\n# -----------------------------\n# 2. ADVANCED FEATURE ENGINEERING\n# -----------------------------\nprint(\"\\n[Step 2/6] Creating advanced engineered features...\")\n\ndef create_advanced_features(df, embedding_df, feature_cols):\n    features_list = []\n    features_list.append(embedding_df)\n    base_features = df[feature_cols].copy()\n    features_list.append(base_features)\n    \n    emb_array = embedding_df.values\n    stat_features = pd.DataFrame({\n        'emb_mean': np.mean(emb_array, axis=1),\n        'emb_std': np.std(emb_array, axis=1),\n        'emb_max': np.max(emb_array, axis=1),\n        'emb_min': np.min(emb_array, axis=1),\n        'emb_median': np.median(emb_array, axis=1),\n        'emb_range': np.max(emb_array, axis=1) - np.min(emb_array, axis=1),\n        'emb_skew': [skew(row) for row in emb_array],\n        'emb_kurtosis': [kurtosis(row) for row in emb_array]\n    })\n    features_list.append(stat_features)\n    \n    if len(feature_cols) > 0:\n        text_array = base_features.values\n        interaction_features = pd.DataFrame({\n            'text_char_word_ratio': df[feature_cols[0]] / (df[feature_cols[1]] + 1) if len(feature_cols) > 1 else 0,\n            'text_complexity_score': df[feature_cols[2]] * df[feature_cols[7]] if len(feature_cols) > 7 else 0,\n            'text_feature_sum': np.sum(text_array, axis=1),\n            'text_feature_mean': np.mean(text_array, axis=1),\n            'text_feature_std': np.std(text_array, axis=1)\n        })\n        features_list.append(interaction_features)\n    \n    if len(feature_cols) > 0:\n        poly_features = pd.DataFrame({\n            'char_count_squared': df[feature_cols[0]] ** 2 if len(feature_cols) > 0 else 0,\n            'word_count_squared': df[feature_cols[1]] ** 2 if len(feature_cols) > 1 else 0,\n            'char_word_interaction': df[feature_cols[0]] * df[feature_cols[1]] if len(feature_cols) > 1 else 0\n        })\n        features_list.append(poly_features)\n    \n    emb_pca_proxy = pd.DataFrame({\n        'emb_first_10_mean': np.mean(emb_array[:, :10], axis=1),\n        'emb_last_10_mean': np.mean(emb_array[:, -10:], axis=1),\n        'emb_first_last_diff': np.mean(emb_array[:, :10], axis=1) - np.mean(emb_array[:, -10:], axis=1)\n    })\n    features_list.append(emb_pca_proxy)\n    \n    combined_features = pd.concat(features_list, axis=1)\n    return combined_features\n\ntrain_features = create_advanced_features(train_df, train_embedding_df, feature_cols)\ntest_features = create_advanced_features(test_df, test_embedding_df, feature_cols)\n\nprint(f\"âœ“ Created {train_features.shape[1]} total features\")\n\ntrain_features = train_features.fillna(0).replace([np.inf, -np.inf], 0)\ntest_features = test_features.fillna(0).replace([np.inf, -np.inf], 0)\n\nscaler = RobustScaler()\ntrain_features_scaled = scaler.fit_transform(train_features)\ntest_features_scaled = scaler.transform(test_features)\n\n# -----------------------------\n# 3. TARGET PREPARATION\n# -----------------------------\ny_orig = train_df[target_col].values.astype(float)\ny_log = np.log1p(y_orig)\n\ndef smape_original_scale(y_true_orig, y_pred_orig):\n    denom = (np.abs(y_true_orig) + np.abs(y_pred_orig))\n    diff = np.abs(y_true_orig - y_pred_orig) / np.where(denom == 0, 1, denom)\n    return 100.0 * np.mean(diff)\n\n# -----------------------------\n# 4. OPTIMIZED MODEL PARAMETERS\n# -----------------------------\nprint(\"\\n[Step 3/6] Initializing optimized models...\")\n\nxgb_params = {\n    'n_estimators': 3000,\n    'learning_rate': 0.04,\n    'max_depth': 7,\n    'min_child_weight': 3,\n    'subsample': 0.85,\n    'colsample_bytree': 0.85,\n    'reg_alpha': 2.0,\n    'reg_lambda': 4.0,\n    'gamma': 0.15,\n    'tree_method': 'hist',\n    'random_state': 42,\n    'n_jobs': -1,\n    'verbosity': 0\n}\n\nlgb_params = {\n    'n_estimators': 3000,\n    'learning_rate': 0.04,\n    'num_leaves': 120,\n    'max_depth': 9,\n    'min_data_in_leaf': 25,\n    'feature_fraction': 0.85,\n    'bagging_fraction': 0.85,\n    'bagging_freq': 5,\n    'lambda_l1': 1.5,\n    'lambda_l2': 3.0,\n    'objective': 'regression_l1',\n    'metric': 'rmse',\n    'random_state': 42,\n    'n_jobs': -1,\n    'verbose': -1\n}\n\ncat_params = {\n    'iterations': 3000,\n    'depth': 9,\n    'learning_rate': 0.035,\n    'l2_leaf_reg': 6.0,\n    'subsample': 0.85,\n    'rsm': 0.85,\n    'random_seed': 42,\n    'loss_function': 'RMSE',\n    'bootstrap_type': 'Bernoulli',\n    'grow_policy': 'Lossguide',\n    'eval_metric': 'RMSE',\n    'verbose': 0\n}\n\nprint(\"âœ“ Models configured with aggressive hyperparameters\")\n\n# -----------------------------\n# 5. SPLIT TRAINING DATA (80/20)\n# -----------------------------\nprint(\"\\n[Step 4/6] Splitting training data (80/20)...\")\nX_train, X_val, y_train_log, y_val_log, y_train_orig, y_val_orig = train_test_split(\n    train_features_scaled, y_log, y_orig, test_size=0.20, random_state=42\n)\nprint(f\"âœ“ Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n\n# -----------------------------\n# 6. PSEUDO-LABELING ON TEST DATA\n# -----------------------------\nprint(\"\\n[Step 5/6] Applying pseudo-labeling for semi-supervised learning...\")\npseudo_X, pseudo_y = pseudo_label_test_data(X_train, y_train_log, test_features_scaled, confidence_threshold=0.75)\n\n# Augment training data\ntrain_features_augmented = np.vstack([X_train, pseudo_X])\ny_log_augmented = np.concatenate([y_train_log, pseudo_y])\nprint(f\"âœ“ Augmented training size: {len(train_features_augmented)} (from {len(X_train)})\")\n\n# -----------------------------\n# 7. TRAIN BASE MODELS AND STACKING META-LEARNERS\n# -----------------------------\nprint(\"\\n[Step 6/6] Training base models and stacking meta-learners...\")\n\nbase_models = [\n    ('XGBoost', XGBRegressor(**xgb_params)),\n    ('LightGBM', LGBMRegressor(**lgb_params)),\n    ('CatBoost', CatBoostRegressor(**cat_params))\n]\n\nmeta_train = np.zeros((len(train_features_augmented), len(base_models)))\nmeta_val = np.zeros((len(X_val), len(base_models)))\nmeta_test = np.zeros((len(test_features_scaled), len(base_models)))\n\nfor i, (name, model) in enumerate(base_models):\n    print(f\"  ðŸ”§ Training {name}...\", end=' ')\n    model_start = time.time()\n    try:\n        model.fit(train_features_augmented, y_log_augmented,\n                  eval_set=[(X_val, y_val_log)], early_stopping_rounds=200, verbose=False)\n    except:\n        model.fit(train_features_augmented, y_log_augmented)\n    meta_train[:, i] = model.predict(train_features_augmented)\n    meta_val[:, i] = model.predict(X_val)\n    meta_test[:, i] = model.predict(test_features_scaled)\n    \n    val_pred_orig = np.expm1(meta_val[:, i])\n    model_smape = smape_original_scale(y_val_orig, val_pred_orig)\n    print(f\"SMAPE: {model_smape:.4f} ({time.time()-model_start:.1f}s)\")\n\nprint(f\"  ðŸŽ¯ Training Quantile Regression Meta-Learner...\", end=' ')\nquantile_model = QuantileRegressor(quantile=0.5, alpha=1.0, solver='highs')\nquantile_model.fit(meta_train, y_log_augmented)\n\nridge_model = Ridge(alpha=1.0, random_state=42)\nridge_model.fit(meta_train, y_log_augmented)\nprint(\"âœ“\")\n\n# Blend meta predictions on validation and test sets\nqr_val_pred = quantile_model.predict(meta_val)\nridge_val_pred = ridge_model.predict(meta_val)\nfinal_val_pred_log = 0.6 * qr_val_pred + 0.4 * ridge_val_pred\nval_pred_orig_final = np.expm1(final_val_pred_log)\nfinal_val_smape = smape_original_scale(y_val_orig, val_pred_orig_final)\n\nprint(f\"\\nðŸŽ‰ Final Validation SMAPE after stacking: {final_val_smape:.4f}\")\n\ntest_pred_log = 0.6 * quantile_model.predict(meta_test) + 0.4 * ridge_model.predict(meta_test)\ntest_pred_orig = np.expm1(test_pred_log)\ntest_pred_orig = np.clip(test_pred_orig, np.percentile(y_orig, 1), np.percentile(y_orig, 99))\n\n# -----------------------------\n# 8. SAVE SUBMISSION\n# -----------------------------\nsubmission = pd.DataFrame({\n    \"sample_id\": test_df[id_col],\n    \"price\": test_pred_orig\n})[[\"sample_id\", \"price\"]]\n\nout_fname = \"submission_advanced_optimized_train_test_split.csv\"\nsubmission.to_csv(out_fname, index=False)\n\nprint(f\"\\nâœ… Submission saved as '{out_fname}'\")\nprint(\"\\nðŸ“„ Sample predictions:\")\nprint(submission)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:54:45.004695Z","iopub.execute_input":"2025-10-13T14:54:45.005398Z","iopub.status.idle":"2025-10-13T16:33:48.873994Z","shell.execute_reply.started":"2025-10-13T14:54:45.005373Z","shell.execute_reply":"2025-10-13T16:33:48.873151Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nðŸš€ ADVANCED PRICE PREDICTION PIPELINE - MAXIMUM SMAPE REDUCTION\nâš¡ FAST MODE: 80/20 TRAIN-TEST SPLIT (No K-Fold)\n================================================================================\n\n[Step 1/6] Loading embeddings and base features...\nâœ“ Loaded 1024 embedding dimensions\nâœ“ Loaded 0 text statistics features\n\n[Step 2/6] Creating advanced engineered features...\nâœ“ Created 1035 total features\n\n[Step 3/6] Initializing optimized models...\nâœ“ Models configured with aggressive hyperparameters\n\n[Step 4/6] Splitting training data (80/20)...\nâœ“ Training samples: 60000, Validation samples: 15000\n\n[Step 5/6] Applying pseudo-labeling for semi-supervised learning...\n  âœ“ Generated 18750 pseudo-labels (25.0% of test data)\n  âœ“ Mean confidence: 0.990\nâœ“ Augmented training size: 78750 (from 60000)\n\n[Step 6/6] Training base models and stacking meta-learners...\n  ðŸ”§ Training XGBoost... SMAPE: 28.2934 (1514.1s)\n  ðŸ”§ Training LightGBM... SMAPE: 29.6171 (1182.1s)\n  ðŸ”§ Training CatBoost... SMAPE: 28.7048 (2613.4s)\n  ðŸŽ¯ Training Quantile Regression Meta-Learner... âœ“\n\nðŸŽ‰ Final Validation SMAPE after stacking: 32.5995\n\nâœ… Submission saved as 'submission_advanced_optimized_train_test_split.csv'\n\nðŸ“„ Sample predictions:\n       sample_id      price\n0         100179  15.080209\n1         245611  13.993014\n2         146263  16.314199\n3          95658  11.012821\n4          36806  15.371251\n...          ...        ...\n74995      93616  11.784727\n74996     249434  16.607181\n74997     162217   7.170080\n74998     230487  16.343235\n74999     279477  15.811797\n\n[75000 rows x 2 columns]\n","output_type":"stream"}],"execution_count":11}]}