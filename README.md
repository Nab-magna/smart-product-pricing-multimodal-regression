<div align="center">

# ğŸ›’ Smart Product Pricing Challenge
## ML Solution for E-commerce Price Prediction

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![SMAPE](https://img.shields.io/badge/SMAPE-56.2-red.svg)](https://github.com)
[![Status](https://img.shields.io/badge/Status-Completed-brightgreen.svg)](https://github.com)

*Predicting optimal product prices using advanced NLP and ensemble machine learning*

</div>

---

## ğŸ¯ Project Overview

This project presents a state-of-the-art machine learning solution for the **Smart Product Pricing Challenge**, where the goal is to predict optimal product prices based on product details including text descriptions and specifications. 

<div align="center">

### ğŸ† **Best Performance: 56.2 SMAPE**

*Using E5 embeddings with advanced ensemble methods*

</div>

## ğŸ“Š Challenge Details

| **Aspect** | **Details** |
|------------|-------------|
| ğŸ¯ **Problem** | Predict product prices from catalog content (title, description, IPQ) |
| ğŸ“ˆ **Dataset** | 75K training samples, 75K test samples |
| ğŸ“ **Evaluation** | Symmetric Mean Absolute Percentage Error (SMAPE) |
| ğŸ¥‡ **Best Score** | **56.2 SMAPE** using E5 embeddings + ensemble methods |
| ğŸš« **Constraints** | No external price lookup, MIT/Apache 2.0 license, <8B parameters |

## ğŸ† Model Performance Comparison

<div align="center">

| ğŸ¥‡ **Model** | ğŸ“Š **SMAPE Score** | ğŸ“ **Description** | ğŸ¯ **Status** |
|--------------|-------------------|-------------------|---------------|
| **E5 Embeddings + Ensemble** | **56.2** | **Best performing model** | âœ… **Winner** |
| VILT Model | 57.0 | Vision-Language Transformer | ğŸ¥ˆ Runner-up |
| BERT Large | 59.3 | BERT Large embeddings | ğŸ¥‰ Third |
| SBERT Model | 62.0 | Sentence-BERT embeddings | 4th Place |

</div>

### ğŸ“ˆ Performance Progression
```
SBERT (62.0) â†’ BERT Large (59.3) â†’ VILT (57.0) â†’ E5 Ensemble (56.2)
     â†“              â†“                â†“              â†“
  Baseline    +2.7 improvement  +2.3 improvement  +0.8 improvement
```

## ğŸ› ï¸ Technical Approach

### ğŸ”¤ 1. Text Processing & Embeddings
<div align="center">

| **Component** | **Technology** | **Purpose** |
|---------------|----------------|-------------|
| ğŸ§  **E5 Embeddings** | Microsoft E5 Model | Semantic text understanding |
| ğŸ“Š **Feature Engineering** | Statistical Analysis | Enhanced text representation |
| ğŸ”§ **Text Features** | Custom Extraction | Character, word, sentence metrics |

</div>

**Extracted Features:**
- ğŸ“ Character count, word count, average word length
- ğŸ“„ Sentence count, uppercase ratio, digit ratio  
- ğŸ”¤ Special character ratio, unique word ratio

### ğŸ—ï¸ 2. Model Architecture
<div align="center">

| **Model** | **Type** | **Key Features** | **Role** |
|-----------|----------|------------------|----------|
| ğŸš€ **XGBoost** | Gradient Boosting | Optimized hyperparameters | Primary predictor |
| âš¡ **LightGBM** | Fast Boosting | L1 objective, speed | Secondary predictor |
| ğŸ± **CatBoost** | Categorical Boosting | Advanced features | Tertiary predictor |
| ğŸ“Š **Tweedie LGBM** | Specialized | Price distribution modeling | Specialized predictor |

</div>

### ğŸ¯ 3. Training Strategy
```mermaid
graph TD
    A[Raw Text Data] --> B[E5 Embeddings]
    A --> C[Feature Engineering]
    B --> D[Feature Combination]
    C --> D
    D --> E[Log Transformation]
    E --> F[5-Fold CV]
    F --> G[4-Model Ensemble]
    G --> H[Final Predictions]
```

## ğŸ“ Project Structure

```
ğŸ›’ Smart Product Pricing Challenge/
â”œâ”€â”€ ğŸ“„ README.md                           # ğŸ“– Project documentation
â”œâ”€â”€ ğŸ““ Untitled1.ipynb                     # ğŸš€ Main E5 ensemble implementation
â”œâ”€â”€ ğŸ““ BERT_LARGE.ipynb                    # ğŸ§  BERT Large model (SMAPE: 59.3)
â”œâ”€â”€ ğŸ““ ml-challenge-vilt.ipynb             # ğŸ‘ï¸ VILT model implementation (SMAPE: 57.0)
â”œâ”€â”€ ğŸ generate_e5_embeddings.py           # ğŸ”§ E5 embeddings generation script
â”œâ”€â”€ ğŸ§  train_e5_embeddings.npy            # ğŸ¯ Pre-computed E5 embeddings (training)
â”œâ”€â”€ ğŸ§  test_e5_embeddings.npy             # ğŸ¯ Pre-computed E5 embeddings (test)
â”œâ”€â”€ ğŸ“Š submission_kfold_logblend.csv      # ğŸ† Final predictions
â””â”€â”€ ğŸ“ [train.csv, test.csv]              # ğŸ“ˆ Dataset files 
```

### ğŸ” File Descriptions
| **File** | **Type** | **Description** | **Performance** |
|----------|----------|-----------------|-----------------|
| `README.md` | ğŸ“– Documentation | Complete project overview and setup guide | - |
| `Untitled1.ipynb` | ğŸš€ Main Code | E5 embeddings + ensemble (best model) | **56.2 SMAPE** |
| `BERT_LARGE.ipynb` | ğŸ§  BERT Model | BERT Large embeddings implementation | **59.3 SMAPE** |
| `ml-challenge-vilt.ipynb` | ğŸ‘ï¸ VILT Model | Vision-Language Transformer | **57.0 SMAPE** |
| `generate_e5_embeddings.py` | ğŸ”§ Script | E5 embeddings generation utility | - |
| `*_embeddings.npy` | ğŸ§  Data | Pre-computed E5 embeddings for fast training | - |
| `submission_*.csv` | ğŸ“Š Output | Final predictions in competition format | - |

## ğŸš€ Quick Start

### ğŸ“¦ Prerequisites
```bash
# Core ML libraries
pip install catboost xgboost lightgbm

# NLP and Transformers
pip install sentence-transformers transformers

# Data processing
pip install scikit-learn pandas numpy
```

### ğŸ¯ Usage Steps
<div align="center">

| **Step** | **Action** | **Description** |
|----------|------------|-----------------|
| 1ï¸âƒ£ | ğŸ“ **Load Data** | Place `train.csv` and `test.csv` in project directory |
| 2ï¸âƒ£ | ğŸ§  **Generate Embeddings** | Run `python generate_e5_embeddings.py` to create E5 embeddings |
| 3ï¸âƒ£ | ğŸš€ **Run Training** | Execute `Untitled1.ipynb` for E5 ensemble (best model) |
| 4ï¸âƒ£ | ğŸ”„ **Try Other Models** | Run `BERT_LARGE.ipynb` or `ml-challenge-vilt.ipynb` for comparison |
| 5ï¸âƒ£ | ğŸ“Š **Generate Predictions** | Output saved as `submission_kfold_logblend.csv` |

</div>

### ğŸ’» Key Code Snippet
```python
# ğŸ§  Load E5 embeddings
train_embeddings = np.load("train_e5_embeddings.npy")
test_embeddings = np.load("test_e5_embeddings.npy")

# ğŸ”— Combine with engineered features
train_features = pd.concat([train_embedding_df, train_df[feature_cols]], axis=1)

# ğŸ¯ K-Fold ensemble training
models = {
    "XGBoost": XGBRegressor(n_estimators=2000, learning_rate=0.03, ...),
    "LightGBM": LGBMRegressor(n_estimators=2000, objective="regression_l1", ...),
    "CatBoost": CatBoostRegressor(iterations=2000, depth=10, ...),
    "Tweedie": LGBMRegressor(objective="tweedie", tweedie_variance_power=1.5, ...)
}
```

## ğŸ”§ Model Configuration

<div align="center">

| **Model** | **Key Parameters** | **Optimization** |
|-----------|-------------------|------------------|
| ğŸš€ **XGBoost** | `n_estimators: 2000`, `learning_rate: 0.03`, `max_depth: 6` | `reg_alpha: 2.0`, `reg_lambda: 4.0` |
| âš¡ **LightGBM** | `n_estimators: 2000`, `learning_rate: 0.03`, `num_leaves: 128` | `objective: "regression_l1"` |
| ğŸ± **CatBoost** | `iterations: 2000`, `depth: 10`, `learning_rate: 0.02` | `loss_function: "RMSE"` |
| ğŸ“Š **Tweedie LGBM** | `n_estimators: 2000`, `learning_rate: 0.03` | `tweedie_variance_power: 1.5` |

</div>

## ğŸ“ˆ Results Analysis

<div align="center">

| **Metric** | **Value** | **Description** |
|------------|-----------|-----------------|
| ğŸ¯ **Final OOF SMAPE** | **56.2** | On original price scale |
| â±ï¸ **Training Time** | ~4 hours | 5-fold cross-validation |
| ğŸ“Š **Feature Dimensions** | 1024+ | E5 embeddings + engineered features |
| ğŸ² **Ensemble Strategy** | Mean | 4 model predictions |

</div>

### ğŸ“Š Performance Breakdown
```
ğŸ† Best Model: E5 Embeddings + Ensemble
ğŸ“ˆ SMAPE Score: 56.2
â±ï¸ Training Time: ~4 hours
ğŸ¯ Cross-Validation: 5-fold
ğŸ”§ Models Used: 4 (XGBoost, LightGBM, CatBoost, Tweedie)
```

## ğŸ¯ Key Insights

<div align="center">

| **Insight** | **Impact** | **Evidence** |
|-------------|------------|--------------|
| ğŸ§  **E5 Embeddings Superiority** | High | 56.2 vs 57.0 (VILT) vs 59.3 (BERT) vs 62.0 (SBERT) |
| ğŸ² **Ensemble Benefits** | Medium | Reduced variance through model diversity |
| ğŸ“Š **Log Transformation** | Critical | Handled price distribution skewness |
| ğŸ”§ **Feature Engineering** | High | Statistical features complemented embeddings |
| ğŸ† **Model Progression** | High | Clear improvement from SBERT â†’ BERT â†’ VILT â†’ E5 |

</div>

## ğŸ”® Future Improvements

<div align="center">

| **Improvement** | **Potential Impact** | **Implementation** |
|-----------------|---------------------|-------------------|
| ğŸ–¼ï¸ **Image Features** | High | Vision models for product images |
| ğŸ§  **Advanced Ensembling** | Medium | Neural network stacking |
| âš™ï¸ **Hyperparameter Optimization** | Medium | Bayesian optimization |
| ğŸ¯ **Feature Selection** | Low | Advanced selection techniques |
| ğŸ“ˆ **Data Augmentation** | Medium | Synthetic data generation |

</div>

## ğŸ“ Methodology Summary

```mermaid
graph LR
    A[ğŸ“Š Raw Data] --> B[ğŸ§¹ Preprocessing]
    B --> C[ğŸ§  E5 Embeddings]
    B --> D[ğŸ”§ Feature Engineering]
    C --> E[ğŸ”— Feature Combination]
    D --> E
    E --> F[ğŸ“Š Log Transformation]
    F --> G[ğŸ¯ K-Fold Training]
    G --> H[ğŸ² Ensemble Models]
    H --> I[ğŸ“ˆ Final Predictions]
```

## ğŸ… Competition Compliance

<div align="center">

| **Requirement** | **Status** | **Details** |
|-----------------|------------|-------------|
| ğŸš« **No External Data** | âœ… **Compliant** | Used only provided training data |
| ğŸ“„ **MIT/Apache License** | âœ… **Compliant** | All models meet license requirements |
| ğŸ”¢ **Parameter Limit** | âœ… **Compliant** | All models under 8B parameters |
| ğŸ“Š **Output Format** | âœ… **Compliant** | Exact CSV format matching requirements |

</div>

---

<div align="center">

## ğŸš€ **Ready to Deploy**

*This solution achieved **56.2 SMAPE** using state-of-the-art E5 embeddings and ensemble methods*





</div>
