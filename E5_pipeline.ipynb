{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd5280c-a3fc-4b61-a0a4-1ea1476cc6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost --quiet\n",
    "!pip install XGBoost --quiet\n",
    "!pip install LightGBM --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa666273-b66a-48eb-8a23-0b924417409a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data loaded successfully!\n",
      "Train shape: (75000, 4)\n",
      "Test shape: (75000, 3)\n",
      "Target column: price\n",
      "ID column: sample_id\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Load training and test data ---\n",
    "train_df = pd.read_csv(\"train.csv\")   # replace with correct path if needed\n",
    "test_df  = pd.read_csv(\"test.csv\")    # replace with correct path if needed\n",
    "\n",
    "# --- Define target and ID columns ---\n",
    "target_col = \"price\"       # target variable\n",
    "id_col     = \"sample_id\"   # unique ID column\n",
    "\n",
    "print(\"âœ… Data loaded successfully!\")\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Target column: {target_col}\")\n",
    "print(f\"ID column: {id_col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c345e0e-9827-4014-a2ed-c6311898797b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features prepared. Shape: (75000, 1024)\n",
      "\n",
      "==== Fold 1/5 ====\n",
      "Training XGBoost ... (eval fit failed: XGBModel.fit() got an unexpected keyword argument 'eval_metric') using basic fit.\n",
      "done. SMAPE (orig scale) = 28.3861\n",
      "Training LightGBM ... (eval fit failed: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds') using basic fit.\n",
      "done. SMAPE (orig scale) = 29.0680\n",
      "Training CatBoost ... done. SMAPE (orig scale) = 29.6246\n",
      "Training Tweedie ... (eval fit failed: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds') using basic fit.\n",
      "done. SMAPE (orig scale) = 27.9978\n",
      "Fold 1 ensemble SMAPE (orig scale): 28.5439 (time: 3185.9s)\n",
      "\n",
      "==== Fold 2/5 ====\n",
      "Training XGBoost ... (eval fit failed: XGBModel.fit() got an unexpected keyword argument 'eval_metric') using basic fit.\n",
      "done. SMAPE (orig scale) = 28.2032\n",
      "Training LightGBM ... (eval fit failed: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds') using basic fit.\n",
      "done. SMAPE (orig scale) = 28.8208\n",
      "Training CatBoost ... done. SMAPE (orig scale) = 29.4732\n",
      "Training Tweedie ... (eval fit failed: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds') using basic fit.\n",
      "done. SMAPE (orig scale) = 28.0231\n",
      "Fold 2 ensemble SMAPE (orig scale): 28.3904 (time: 3068.4s)\n",
      "\n",
      "==== Fold 3/5 ====\n",
      "Training XGBoost ... (eval fit failed: XGBModel.fit() got an unexpected keyword argument 'eval_metric') using basic fit.\n",
      "done. SMAPE (orig scale) = 28.2052\n",
      "Training LightGBM ... (eval fit failed: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds') using basic fit.\n",
      "done. SMAPE (orig scale) = 28.7066\n",
      "Training CatBoost ... done. SMAPE (orig scale) = 29.3886\n",
      "Training Tweedie ... (eval fit failed: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds') using basic fit.\n",
      "done. SMAPE (orig scale) = 27.9704\n",
      "Fold 3 ensemble SMAPE (orig scale): 28.3443 (time: 3088.3s)\n",
      "\n",
      "==== Fold 4/5 ====\n",
      "Training XGBoost ... (eval fit failed: XGBModel.fit() got an unexpected keyword argument 'eval_metric') using basic fit.\n",
      "done. SMAPE (orig scale) = 27.9129\n",
      "Training LightGBM ... (eval fit failed: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds') using basic fit.\n",
      "done. SMAPE (orig scale) = 28.4271\n",
      "Training CatBoost ... done. SMAPE (orig scale) = 28.9700\n",
      "Training Tweedie ... (eval fit failed: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds') using basic fit.\n",
      "done. SMAPE (orig scale) = 27.6362\n",
      "Fold 4 ensemble SMAPE (orig scale): 27.9830 (time: 3067.4s)\n",
      "\n",
      "==== Fold 5/5 ====\n",
      "Training XGBoost ... (eval fit failed: XGBModel.fit() got an unexpected keyword argument 'eval_metric') using basic fit.\n",
      "done. SMAPE (orig scale) = 28.2785\n",
      "Training LightGBM ... (eval fit failed: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds') using basic fit.\n",
      "done. SMAPE (orig scale) = 28.8374\n",
      "Training CatBoost ... done. SMAPE (orig scale) = 29.3605\n",
      "Training Tweedie ... (eval fit failed: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds') using basic fit.\n",
      "done. SMAPE (orig scale) = 27.9466\n",
      "Fold 5 ensemble SMAPE (orig scale): 28.3675 (time: 3058.3s)\n",
      "\n",
      "============================================================\n",
      "Final OOF SMAPE (orig price scale): 28.3258\n",
      "Fold SMAPEs mean: 28.3258 Â± 0.1851\n",
      "Total training time: 15577.6s\n",
      "============================================================\n",
      "\n",
      "âœ… Submission file saved as 'submission_kfold_logblend.csv'\n",
      "ðŸ“„ First few rows of submission:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100179</td>\n",
       "      <td>13.815932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245611</td>\n",
       "      <td>11.742197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>146263</td>\n",
       "      <td>25.263652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95658</td>\n",
       "      <td>10.819246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36806</td>\n",
       "      <td>19.624214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id      price\n",
       "0     100179  13.815932\n",
       "1     245611  11.742197\n",
       "2     146263  25.263652\n",
       "3      95658  10.819246\n",
       "4      36806  19.624214"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# K-FOLD ENSEMBLE: XGBoost + LightGBM + CatBoost + Tweedie-LGBM\n",
    "# (log-target training, SMAPE computed on original price scale)\n",
    "# ===================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# Ensure these variables exist:\n",
    "# train_df, test_df, target_col, id_col\n",
    "# Also provide paths to embedding files:\n",
    "train_emb_file = \"train_e5_embeddings.npy\"  # <-- update path if needed\n",
    "test_emb_file  = \"test_e5_embeddings.npy\"   # <-- update path if needed\n",
    "# -----------------------------\n",
    "\n",
    "# Load embeddings from files\n",
    "train_embeddings = np.load(train_emb_file)\n",
    "test_embeddings  = np.load(test_emb_file)\n",
    "\n",
    "# Create embedding DataFrames\n",
    "train_embedding_df = pd.DataFrame(train_embeddings, columns=[f'e5_emb_{i}' for i in range(train_embeddings.shape[1])])\n",
    "test_embedding_df  = pd.DataFrame(test_embeddings,  columns=[f'e5_emb_{i}' for i in range(test_embeddings.shape[1])])\n",
    "\n",
    "# Combine embeddings with your text-stat features (same feature selection you used)\n",
    "feature_cols = [col for col in train_df.columns if any(\n",
    "    suffix in col for suffix in ['_char_count', '_word_count', '_avg_word_length', \n",
    "                                 '_sentence_count', '_uppercase_ratio', '_digit_ratio',\n",
    "                                 '_special_char_ratio', '_unique_word_ratio']\n",
    ")]\n",
    "\n",
    "train_features = pd.concat([train_embedding_df, train_df[feature_cols].reset_index(drop=True)], axis=1)\n",
    "test_features  = pd.concat([test_embedding_df,  test_df[feature_cols].reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Fill missing and scale\n",
    "train_features = train_features.fillna(0)\n",
    "test_features  = test_features.fillna(0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled  = scaler.transform(test_features)\n",
    "\n",
    "print(f\"Features prepared. Shape: {train_features_scaled.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Target: use log1p for training stability\n",
    "# -----------------------------\n",
    "y_orig = train_df[target_col].values.astype(float)    # original prices\n",
    "y_log  = np.log1p(y_orig)                             # log-transformed target used for training\n",
    "\n",
    "# -----------------------------\n",
    "# Define models (kept as requested)\n",
    "# -----------------------------\n",
    "models = {\n",
    "    \"XGBoost\": XGBRegressor(\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=6,\n",
    "        min_child_weight=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=2.0,\n",
    "        reg_lambda=4.0,\n",
    "        gamma=0.2,\n",
    "        tree_method=\"hist\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    ),\n",
    "\n",
    "    \"LightGBM\": LGBMRegressor(\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=128,\n",
    "        max_depth=8,\n",
    "        min_data_in_leaf=40,\n",
    "        feature_fraction=0.8,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=5,\n",
    "        lambda_l1=1.0,\n",
    "        lambda_l2=2.0,\n",
    "        objective=\"regression_l1\",\n",
    "        metric=\"rmse\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    ),\n",
    "\n",
    "    \"CatBoost\": CatBoostRegressor(\n",
    "        iterations=2000,\n",
    "        depth=10,\n",
    "        learning_rate=0.02,\n",
    "        l2_leaf_reg=6.0,\n",
    "        random_seed=42,\n",
    "        loss_function=\"RMSE\",\n",
    "        bootstrap_type=\"Bernoulli\",\n",
    "        subsample=0.7,\n",
    "        rsm=0.8,\n",
    "        grow_policy=\"Lossguide\",\n",
    "        eval_metric=\"RMSE\",\n",
    "        verbose=0\n",
    "    ),\n",
    "\n",
    "    \"Tweedie\": LGBMRegressor(\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=128,\n",
    "        max_depth=8,\n",
    "        min_data_in_leaf=40,\n",
    "        feature_fraction=0.8,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=5,\n",
    "        lambda_l1=1.0,\n",
    "        lambda_l2=2.0,\n",
    "        objective=\"tweedie\",\n",
    "        tweedie_variance_power=1.5,\n",
    "        metric=\"rmse\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# SMAPE computed on original scale\n",
    "# -----------------------------\n",
    "def smape_original_scale(y_true_orig, y_pred_orig):\n",
    "    denom = (np.abs(y_true_orig) + np.abs(y_pred_orig))\n",
    "    diff = np.abs(y_true_orig - y_pred_orig) / np.where(denom == 0, 1, denom)\n",
    "    return 100.0 * np.mean(diff)\n",
    "\n",
    "# -----------------------------\n",
    "# K-Fold training\n",
    "# -----------------------------\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "oof_preds_log = np.zeros(len(train_features_scaled))\n",
    "test_preds_log = np.zeros((len(test_features_scaled), len(models)))\n",
    "fold_scores = []\n",
    "\n",
    "start_time = time.time()\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(train_features_scaled), 1):\n",
    "    t0 = time.time()\n",
    "    print(f\"\\n==== Fold {fold}/{n_splits} ====\")\n",
    "    X_tr, X_val = train_features_scaled[tr_idx], train_features_scaled[val_idx]\n",
    "    y_tr_log, y_val_log = y_log[tr_idx], y_log[val_idx]\n",
    "    y_val_orig = y_orig[val_idx]\n",
    "\n",
    "    fold_model_val_preds_log = np.zeros((len(val_idx), len(models)))\n",
    "    \n",
    "    for m_idx, (name, model) in enumerate(models.items()):\n",
    "        print(f\"Training {name} ...\", end=' ')\n",
    "        try:\n",
    "            if name == \"XGBoost\":\n",
    "                model.fit(\n",
    "                    X_tr, y_tr_log,\n",
    "                    eval_set=[(X_val, y_val_log)],\n",
    "                    eval_metric=\"rmse\",\n",
    "                    early_stopping_rounds=100,\n",
    "                    verbose=False\n",
    "                )\n",
    "            elif name in (\"LightGBM\", \"Tweedie\"):\n",
    "                model.fit(\n",
    "                    X_tr, y_tr_log,\n",
    "                    eval_set=[(X_val, y_val_log)],\n",
    "                    eval_metric=\"rmse\",\n",
    "                    early_stopping_rounds=100,\n",
    "                    verbose=False\n",
    "                )\n",
    "            elif name == \"CatBoost\":\n",
    "                model.fit(\n",
    "                    X_tr, y_tr_log,\n",
    "                    eval_set=(X_val, y_val_log),\n",
    "                    use_best_model=True,\n",
    "                    early_stopping_rounds=100,\n",
    "                    verbose=False\n",
    "                )\n",
    "            else:\n",
    "                model.fit(X_tr, y_tr_log)\n",
    "        except Exception as e:\n",
    "            print(f\"(eval fit failed: {e}) using basic fit.\")\n",
    "            model.fit(X_tr, y_tr_log)\n",
    "        \n",
    "        val_pred_log = model.predict(X_val)\n",
    "        fold_model_val_preds_log[:, m_idx] = val_pred_log\n",
    "        \n",
    "        val_pred_orig = np.expm1(val_pred_log)\n",
    "        model_smape = smape_original_scale(y_val_orig, val_pred_orig)\n",
    "        print(f\"done. SMAPE (orig scale) = {model_smape:.4f}\")\n",
    "    \n",
    "    val_pred_mean_log = np.mean(fold_model_val_preds_log, axis=1)\n",
    "    oof_preds_log[val_idx] = val_pred_mean_log\n",
    "    \n",
    "    val_pred_mean_orig = np.expm1(val_pred_mean_log)\n",
    "    fold_smape = smape_original_scale(y_val_orig, val_pred_mean_orig)\n",
    "    fold_scores.append(fold_smape)\n",
    "    print(f\"Fold {fold} ensemble SMAPE (orig scale): {fold_smape:.4f} (time: {time.time()-t0:.1f}s)\")\n",
    "\n",
    "    test_fold_preds = np.column_stack([model.predict(test_features_scaled) for model in models.values()])\n",
    "    test_preds_log += test_fold_preds / n_splits\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "oof_preds_orig = np.expm1(oof_preds_log)\n",
    "final_oof_smape = smape_original_scale(y_orig, oof_preds_orig)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Final OOF SMAPE (orig price scale): {final_oof_smape:.4f}\")\n",
    "print(f\"Fold SMAPEs mean: {np.mean(fold_scores):.4f} Â± {np.std(fold_scores):.4f}\")\n",
    "print(f\"Total training time: {total_time:.1f}s\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_pred_final_orig = np.expm1(np.mean(test_preds_log, axis=1))\n",
    "\n",
    "\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"sample_id\": test_df[id_col],\n",
    "    \"price\": test_pred_final_orig\n",
    "})[[\"sample_id\", \"price\"]]\n",
    "\n",
    "out_fname = \"submission_kfold_logblend.csv\"\n",
    "submission.to_csv(out_fname, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Submission file saved as '{out_fname}'\")\n",
    "print(\"ðŸ“„ First few rows of submission:\")\n",
    "display(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6317a9a6-14d7-4cb1-ad1d-47c45ebdf7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
